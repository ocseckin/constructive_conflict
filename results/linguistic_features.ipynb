{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pointbiserialr\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
    "from lexical_diversity import lex_div as ld\n",
    "from collections import Counter\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.serif'] = ['Helvetica'] + plt.rcParams['font.serif']\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "plt.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/submissions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the test set for analysis\n",
    "df = df[df['train_valid_test'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break Submissions into Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_body_sentences'] = df['clean_body'].apply(lambda x: sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_body_tokens'] = df['clean_body'].apply(lambda x: word_tokenize(x))\n",
    "df['clean_body_tokens'] = df['clean_body_tokens'].apply(lambda x: [i for i in x if i!=''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question marks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_question'] = df['clean_body_sentences'].apply(lambda x: 1 if '?' in x else 0)\n",
    "df['question_ratio'] = df['clean_body_sentences'].apply(lambda x: len([i for i in x if '?' in i])/len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gratitude**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = {\"thanks\", \"contented\", \"blessed\", \"thank you\", \"thankful for\", \"grateful for\", \"greatful for\", \"my gratitude\", \"i appreciate\", \"made me smile\", \"make me smile\", \"i super appreciate\", \"i deeply appreciate\", \"i really appreciate\", \"bless your soul\", \"made my day\", \"tysm\", \"thx\", \"shout out to\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary\n",
    "df['gratitude'] = df['clean_body'].apply(lambda x: 1 if any([i in x.lower() for i in lexicon]) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the counts\n",
    "df['gratitude_count'] = df['clean_body'].apply(lambda x: sum([x.lower().count(w) for w in lexicon]))\n",
    "\n",
    "# get the ratio\n",
    "df['gratitude_ratio'] = df['gratitude_count'] / df['clean_body_tokens'].apply(lambda x: len(x) if len(x) > 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Proper Nouns from POS Tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tags'] = df['clean_body'].apply(lambda x: pos_tag(word_tokenize(x, language='english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pos_tag_counts'] = df['pos_tags'].apply(lambda x: Counter([i[1] for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['proper_noun_count'] = df['pos_tag_counts'].apply(lambda x: x.get(\"NNP\",0) + x.get(\"NNPS\",0))\n",
    "\n",
    "df['proper_noun_ratio'] = df['proper_noun_count'] / df['pos_tags'].apply(lambda x: len(x) if len(x) > 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Elaboration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['pos_tag_counts'].apply(lambda x: sum(x.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_items = {'NN', 'NNS', 'NNP', 'NNPS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS'}\n",
    "\n",
    "df['lexical_item_count'] = df['pos_tags'].apply(lambda x: len(set([t for t in x if t[1] in lexical_items])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_body_tokens'] = df['clean_body'].apply(lambda x: ld.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['mtld'] = df['clean_body_tokens'].apply(lambda x: ld.mtld(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hedge Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = []\n",
    "# read the files and dump the contents into the list \"lexicon\"\n",
    "for _ in glob.glob('../data/hedge_resources/*'):\n",
    "    name = _.split('/')[-1].split('.')[0]\n",
    "    with open(_, 'r') as f:\n",
    "        temp = f.read().splitlines()\n",
    "        temp = [i for i in temp if i!='' and i[0]!='#']\n",
    "\n",
    "        if name == 'booster_words':\n",
    "            temp = [f\"not {i}\" for i in temp] + [f\"without {i}\" for i in temp]\n",
    "        lexicon.extend(temp)\n",
    "\n",
    "# lowercase the lexicon\n",
    "lexicon = [i.lower() for i in lexicon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hedge'] = df['clean_body'].apply(lambda x: 1 if any([i in x.lower() for i in lexicon]) else 0)\n",
    "\n",
    "# get the counts\n",
    "df['hedge_count'] = df['clean_body'].apply(lambda x: sum([x.lower().count(w) for w in lexicon]))\n",
    "\n",
    "df['hedge_ratio'] = df['hedge_count'] / df['pos_tags'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['polarity'] = df['clean_body'].apply(lambda x: sid.polarity_scores(x)['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive_polarity'] = 0\n",
    "df['negative_polarity'] = 0\n",
    "df.loc[df['polarity']>0, 'positive_polarity'] = 1\n",
    "df.loc[df['polarity']<0, 'negative_polarity'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_median = df['c_score'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER = df['c_score']>c_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.560849985667126"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[FILTER]['hedge'].sum() / len(df[FILTER])\n",
    "df[~FILTER]['hedge'].sum() / len(df[~FILTER])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_question\n",
      "controversial: SignificanceResult(statistic=-0.2997438110394871, pvalue=0.0)\n",
      "percentage of occurrence: 40.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.2713689094556239, pvalue=0.0)\n",
      "percentage of occurrence: 43.0%\n",
      "-----------------------------------\n",
      "gratitude\n",
      "controversial: SignificanceResult(statistic=-0.026114766055494044, pvalue=7.435967524729462e-10)\n",
      "percentage of occurrence: 3.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.05178714486500907, pvalue=2.589252454852214e-34)\n",
      "percentage of occurrence: 5.0%\n",
      "-----------------------------------\n",
      "hedge\n",
      "controversial: SignificanceResult(statistic=-0.10210276705398079, pvalue=1.2236794759213476e-128)\n",
      "percentage of occurrence: 52.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.14696688144137235, pvalue=8.367918990359034e-266)\n",
      "percentage of occurrence: 52.0%\n",
      "-----------------------------------\n",
      "positive_polarity\n",
      "controversial: SignificanceResult(statistic=-0.10166432547299345, pvalue=1.5089553959678863e-127)\n",
      "percentage of occurrence: 33.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.1250751583292331, pvalue=1.6060142812739227e-192)\n",
      "percentage of occurrence: 46.0%\n",
      "-----------------------------------\n",
      "negative_polarity\n",
      "controversial: SignificanceResult(statistic=0.11121813068702646, pvalue=2.0932743157783616e-152)\n",
      "percentage of occurrence: 48.0%\n",
      "non-controversial: SignificanceResult(statistic=0.08226318163108802, pvalue=4.926814183238871e-84)\n",
      "percentage of occurrence: 26.0%\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for col in ['is_question', 'gratitude', 'hedge', 'positive_polarity', 'negative_polarity']:\n",
    "    print(col)\n",
    "    print(\"controversial:\", pointbiserialr(df[FILTER][col], df[FILTER]['ta_score']))\n",
    "    print(f'percentage of occurrence: {round(df[FILTER][col].sum()/len(df[FILTER]),2)*100}%')\n",
    "    \n",
    "    print(\"non-controversial:\", pointbiserialr(df[~FILTER][col], df[~FILTER]['ta_score']))\n",
    "    print(f'percentage of occurrence: {round(df[~FILTER][col].sum()/len(df[~FILTER]),2)*100}%')\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question_ratio\n",
      "controversial: SignificanceResult(statistic=-0.4389331973250033, pvalue=0.0)\n",
      "percentage of occurrence: 47.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.3786187406989999, pvalue=0.0)\n",
      "percentage of occurrence: 50.0%\n",
      "-----------------------------------\n",
      "gratitude_ratio\n",
      "controversial: SignificanceResult(statistic=-0.06333147156947244, pvalue=1.6124441617310633e-96)\n",
      "percentage of occurrence: 3.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.10826713691935491, pvalue=2.9334393419763457e-279)\n",
      "percentage of occurrence: 5.0%\n",
      "-----------------------------------\n",
      "proper_noun_ratio\n",
      "controversial: SignificanceResult(statistic=0.35727541911536714, pvalue=0.0)\n",
      "percentage of occurrence: 81.0%\n",
      "non-controversial: SignificanceResult(statistic=0.002093977937221086, pvalue=0.4910752747427096)\n",
      "percentage of occurrence: 64.0%\n",
      "-----------------------------------\n",
      "text_length\n",
      "controversial: SignificanceResult(statistic=-0.24512543007594223, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.32268209282480553, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "-----------------------------------\n",
      "lexical_item_count\n",
      "controversial: SignificanceResult(statistic=-0.18535791457233194, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.30785255610363454, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "-----------------------------------\n",
      "mtld\n",
      "controversial: SignificanceResult(statistic=-0.2393051576392076, pvalue=0.0)\n",
      "percentage of occurrence: 59.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.3001515077686434, pvalue=0.0)\n",
      "percentage of occurrence: 61.0%\n",
      "-----------------------------------\n",
      "submission_openai\n",
      "controversial: SignificanceResult(statistic=0.2799930421881797, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "non-controversial: SignificanceResult(statistic=0.17891116486173064, pvalue=0.0)\n",
      "percentage of occurrence: 100.0%\n",
      "-----------------------------------\n",
      "polarity\n",
      "controversial: SignificanceResult(statistic=-0.16533859953178762, pvalue=0.0)\n",
      "percentage of occurrence: 81.0%\n",
      "non-controversial: SignificanceResult(statistic=-0.2283288744134071, pvalue=0.0)\n",
      "percentage of occurrence: 74.0%\n",
      "-----------------------------------\n",
      "hedge_ratio\n",
      "controversial: SignificanceResult(statistic=-0.15992791614437551, pvalue=0.0)\n",
      "percentage of occurrence: 55.00000000000001%\n",
      "non-controversial: SignificanceResult(statistic=-0.16935395153419225, pvalue=0.0)\n",
      "percentage of occurrence: 56.00000000000001%\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "for col in ['question_ratio', 'gratitude_ratio', 'proper_noun_ratio', 'text_length', 'lexical_item_count', 'mtld', 'submission_openai', 'polarity', 'hedge_ratio']:\n",
    "    print(col)\n",
    "    print(\"controversial:\", spearmanr(df[FILTER][col], df[FILTER]['ta_score']))\n",
    "    print(f'percentage of occurrence: {round((df[FILTER][col]!=0).sum()/len(df[FILTER]),2)*100}%')\n",
    "    print(\"non-controversial:\", spearmanr(df[~FILTER][col], df[~FILTER]['ta_score']))\n",
    "    print(f'percentage of occurrence: {round((df[~FILTER][col]!=0).sum()/len(df[~FILTER]),2)*100}%')\n",
    "    print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Between Elaboration Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.7980426656494866, pvalue=0.0)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(df['mtld'], df['lexical_item_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=0.9779860625958424, pvalue=0.0)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(df['text_length'], df['lexical_item_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variation Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c_score</td>\n",
       "      <td>2.467624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>question_ratio</td>\n",
       "      <td>1.470034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gratitude_ratio</td>\n",
       "      <td>1.005694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>proper_noun_ratio</td>\n",
       "      <td>1.600044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lexical_item_count</td>\n",
       "      <td>1.163930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hedge_ratio</td>\n",
       "      <td>1.400745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>polarity</td>\n",
       "      <td>1.054376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              feature       VIF\n",
       "0             c_score  2.467624\n",
       "1      question_ratio  1.470034\n",
       "2     gratitude_ratio  1.005694\n",
       "3   proper_noun_ratio  1.600044\n",
       "4  lexical_item_count  1.163930\n",
       "5         hedge_ratio  1.400745\n",
       "6            polarity  1.054376"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the independent variables set\n",
    "X = df[['c_score','question_ratio','gratitude_ratio','proper_noun_ratio','lexical_item_count','hedge_ratio','polarity']]\n",
    "\n",
    "# VIF dataframe\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "\n",
    "# calculating VIF for each feature\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n",
    "                          for i in range(len(X.columns))]\n",
    "\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>ta_score</td>     <th>  R-squared:         </th>  <td>   0.371</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.371</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>1.819e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 22 Apr 2025</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:12:12</td>     <th>  Log-Likelihood:    </th> <td>2.4848e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>216286</td>      <th>  AIC:               </th> <td>-4.969e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>216278</td>      <th>  BIC:               </th> <td>-4.969e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "           <td></td>             <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>              <td>    0.0537</td> <td>    0.000</td> <td>  128.575</td> <td> 0.000</td> <td>    0.053</td> <td>    0.055</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>c_score</th>            <td>    0.3437</td> <td>    0.001</td> <td>  264.250</td> <td> 0.000</td> <td>    0.341</td> <td>    0.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>question_ratio</th>     <td>   -0.0667</td> <td>    0.001</td> <td> -130.057</td> <td> 0.000</td> <td>   -0.068</td> <td>   -0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gratitude_ratio</th>    <td>   -0.0175</td> <td>    0.022</td> <td>   -0.800</td> <td> 0.424</td> <td>   -0.060</td> <td>    0.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>proper_noun_ratio</th>  <td>    0.0461</td> <td>    0.001</td> <td>   43.530</td> <td> 0.000</td> <td>    0.044</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lexical_item_count</th> <td> 1.001e-06</td> <td> 1.45e-06</td> <td>    0.692</td> <td> 0.489</td> <td>-1.83e-06</td> <td> 3.84e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>hedge_ratio</th>        <td>    0.0172</td> <td>    0.004</td> <td>    4.886</td> <td> 0.000</td> <td>    0.010</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>polarity</th>           <td>   -0.0185</td> <td>    0.000</td> <td>  -61.206</td> <td> 0.000</td> <td>   -0.019</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>28978.938</td> <th>  Durbin-Watson:     </th> <td>   1.534</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>44342.610</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.966</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.091</td>   <th>  Cond. No.          </th> <td>1.68e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.68e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}       &    ta\\_score     & \\textbf{  R-squared:         } &     0.371   \\\\\n",
       "\\textbf{Model:}               &       OLS        & \\textbf{  Adj. R-squared:    } &     0.371   \\\\\n",
       "\\textbf{Method:}              &  Least Squares   & \\textbf{  F-statistic:       } & 1.819e+04   \\\\\n",
       "\\textbf{Date:}                & Tue, 22 Apr 2025 & \\textbf{  Prob (F-statistic):} &     0.00    \\\\\n",
       "\\textbf{Time:}                &     21:12:12     & \\textbf{  Log-Likelihood:    } & 2.4848e+05  \\\\\n",
       "\\textbf{No. Observations:}    &      216286      & \\textbf{  AIC:               } & -4.969e+05  \\\\\n",
       "\\textbf{Df Residuals:}        &      216278      & \\textbf{  BIC:               } & -4.969e+05  \\\\\n",
       "\\textbf{Df Model:}            &           7      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}     &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                              & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}                &       0.0537  &        0.000     &   128.575  &         0.000        &        0.053    &        0.055     \\\\\n",
       "\\textbf{c\\_score}             &       0.3437  &        0.001     &   264.250  &         0.000        &        0.341    &        0.346     \\\\\n",
       "\\textbf{question\\_ratio}      &      -0.0667  &        0.001     &  -130.057  &         0.000        &       -0.068    &       -0.066     \\\\\n",
       "\\textbf{gratitude\\_ratio}     &      -0.0175  &        0.022     &    -0.800  &         0.424        &       -0.060    &        0.025     \\\\\n",
       "\\textbf{proper\\_noun\\_ratio}  &       0.0461  &        0.001     &    43.530  &         0.000        &        0.044    &        0.048     \\\\\n",
       "\\textbf{lexical\\_item\\_count} &    1.001e-06  &     1.45e-06     &     0.692  &         0.489        &    -1.83e-06    &     3.84e-06     \\\\\n",
       "\\textbf{hedge\\_ratio}         &       0.0172  &        0.004     &     4.886  &         0.000        &        0.010    &        0.024     \\\\\n",
       "\\textbf{polarity}             &      -0.0185  &        0.000     &   -61.206  &         0.000        &       -0.019    &       -0.018     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 28978.938 & \\textbf{  Durbin-Watson:     } &     1.534  \\\\\n",
       "\\textbf{Prob(Omnibus):} &    0.000  & \\textbf{  Jarque-Bera (JB):  } & 44342.610  \\\\\n",
       "\\textbf{Skew:}          &    0.966  & \\textbf{  Prob(JB):          } &      0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &    4.091  & \\textbf{  Cond. No.          } &  1.68e+04  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.68e+04. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               ta_score   R-squared:                       0.371\n",
       "Model:                            OLS   Adj. R-squared:                  0.371\n",
       "Method:                 Least Squares   F-statistic:                 1.819e+04\n",
       "Date:                Tue, 22 Apr 2025   Prob (F-statistic):               0.00\n",
       "Time:                        21:12:12   Log-Likelihood:             2.4848e+05\n",
       "No. Observations:              216286   AIC:                        -4.969e+05\n",
       "Df Residuals:                  216278   BIC:                        -4.969e+05\n",
       "Df Model:                           7                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================\n",
       "                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------\n",
       "const                  0.0537      0.000    128.575      0.000       0.053       0.055\n",
       "c_score                0.3437      0.001    264.250      0.000       0.341       0.346\n",
       "question_ratio        -0.0667      0.001   -130.057      0.000      -0.068      -0.066\n",
       "gratitude_ratio       -0.0175      0.022     -0.800      0.424      -0.060       0.025\n",
       "proper_noun_ratio      0.0461      0.001     43.530      0.000       0.044       0.048\n",
       "lexical_item_count  1.001e-06   1.45e-06      0.692      0.489   -1.83e-06    3.84e-06\n",
       "hedge_ratio            0.0172      0.004      4.886      0.000       0.010       0.024\n",
       "polarity              -0.0185      0.000    -61.206      0.000      -0.019      -0.018\n",
       "==============================================================================\n",
       "Omnibus:                    28978.938   Durbin-Watson:                   1.534\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            44342.610\n",
       "Skew:                           0.966   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.091   Cond. No.                     1.68e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.68e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare X and y\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "y = df['ta_score']\n",
    "\n",
    "# initialize and fit the model\n",
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "\n",
    "# see the results\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Qualitative Analysis** (Table 2)\n",
    "\n",
    "### Find BERTopic topic names associated with highly controversial topics\n",
    "Topics are chosen from 2023 Gallup survey (https://news.gallup.com/poll/509129/update-partisan-gaps-expand-government-power-climate.aspx)\n",
    "- Abortion\n",
    "- Healthcare\n",
    "- Gun Laws\n",
    "- LGBTQ+\n",
    "- Climate Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion:\n",
      "bertopic\n",
      "18_abortion_ban_rights_texas          406\n",
      "257_walker_georgia_senate_abortion     40\n",
      "Name: count, dtype: int64\n",
      "----------------\n",
      "healthcare:\n",
      "bertopic\n",
      "54_insurance_healthcare_health_medical    173\n",
      "Name: count, dtype: int64\n",
      "----------------\n",
      "medicare:\n",
      "bertopic\n",
      "271_medicare_medicaid_expansion_coverage    37\n",
      "Name: count, dtype: int64\n",
      "----------------\n",
      "guns:\n",
      "bertopic\n",
      "17_gun_guns_shootings_shooting    412\n",
      "Name: count, dtype: int64\n",
      "----------------\n",
      "gay:\n",
      "bertopic\n",
      "71_gender_gay_nonbinary_bisexual       141\n",
      "223_cup_world_football_gay              45\n",
      "234_cases_outbreak_monkey_gay           44\n",
      "329_samesex_marriage_gay_court          30\n",
      "622_homophobic_gay_homophobia_queer     14\n",
      "Name: count, dtype: int64\n",
      "----------------\n",
      "climate:\n",
      "bertopic\n",
      "32_climate_warming_change_global       258\n",
      "135_climate_heat_hottest_warming        75\n",
      "728_activists_climate_painting_pour     12\n",
      "771_activists_climate_traffic_block     11\n",
      "Name: count, dtype: int64\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "for keyword in ['abortion', 'healthcare', 'medicare', 'guns', 'gay', 'climate']:\n",
    "    print(f\"{keyword}:\")\n",
    "    print(df[df['bertopic'].apply(lambda x: keyword in x)]['bertopic'].value_counts())\n",
    "    print('----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18_abortion_ban_rights_texas\n",
      "High TA: Cherokee Nation: Governor’s claim of ‘abortion on-demand’ on Tribal lands is ‘irresponsible’ \n",
      "--------\n",
      "Low TA: How soon could US states outlaw abortions if Roe v Wade is overturned? | Roe v Wade | The Guardian \n",
      "--------\n",
      "\n",
      "271_medicare_medicaid_expansion_coverage\n",
      "High TA: GOP dominated Wisconsin legislature yet again rejects ACA Medicaid expansion for WI, (the only Midwestern state to reject it), because someone on Medicaid might win the lottery but still remain on Medicaid \n",
      "--------\n",
      "Low TA: Democrats want to rescue union pensions from the party's failed bailout plan \n",
      "--------\n",
      "\n",
      "17_gun_guns_shootings_shooting\n",
      "High TA: They Want to Take Your Guns; CBS Cheers Canada’s Mass Gun Bans, Confiscation Plans \n",
      "--------\n",
      "Low TA: If guns aren’t the issue, people are, then wouldn’t that somewhat insinuate that we’re purposefully giving the wrong people guns? Asking because that’s how I’m starting to view it. I’m more than likely missing something here so I’d like someone to inform me so I don’t feel like I’m clueless. Would appreciate if explained politely. \n",
      "--------\n",
      "\n",
      "71_gender_gay_nonbinary_bisexual\n",
      "High TA: All straight men would suck their own cock if they could \n",
      "--------\n",
      "Low TA: 72 Genders? Is this consensus in the US? I came across a list of 72 different genders. On this list were terms like \"ceterogender\", \"esspigender\" and many more. Do people really believe in this? \n",
      "--------\n",
      "\n",
      "32_climate_warming_change_global\n",
      "High TA: U.N. experts urge stringent rules to stop net zero greenwash \n",
      "--------\n",
      "Low TA: Rewilding animals could be key for climate: Report \n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prepare the filter to find high and low TA submissions\n",
    "ta_median = df['ta_score'].median()\n",
    "TA_FILTER = df['ta_score']>ta_median\n",
    "\n",
    "for bertopic in ['18_abortion_ban_rights_texas', '271_medicare_medicaid_expansion_coverage', '17_gun_guns_shootings_shooting','71_gender_gay_nonbinary_bisexual', '32_climate_warming_change_global']:\n",
    "    \n",
    "    # given the topic, filter out the submissions\n",
    "    BERTOPIC_FILTER = df['bertopic'].apply(lambda x: bertopic in x)\n",
    "\n",
    "    # see examples combining both filters\n",
    "    print(bertopic)\n",
    "    print(\"High TA:\", df[BERTOPIC_FILTER & TA_FILTER]['clean_body'].sample(1).iloc[0], '\\n--------')\n",
    "    print(\"Low TA:\", df[BERTOPIC_FILTER & ~TA_FILTER]['clean_body'].sample(1).iloc[0], '\\n--------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prosocial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
